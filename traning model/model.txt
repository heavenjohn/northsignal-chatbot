# I use google colab

!pip install tensorflow
!pip install nltk
!pip install colorama
!pip inatall numpy
!pip install scikit_learn
!pip install Flask
!pip install tensorflowjs

import nltk
import numpy as np
import json
import random
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import tensorflow as tf
import pickle
import re

# Download required NLTK data
nltk.download('punkt')
nltk.download('wordnet')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Load the intents file
with open('/content/drive/MyDrive/Intents/Intents.json') as file:
    intents = json.load(file)

# Initialize data structures
training_sentences = []
training_labels = []
classes = []
words = []

# Process intents
for intent in intents['intents']:
    for pattern in intent['patterns']:
        word_list = re.findall(r'\b\w+\b', pattern)
        words.extend(word_list)
        training_sentences.append(pattern)
        training_labels.append(intent['tag'])
    if intent['tag'] not in classes:
        classes.append(intent['tag'])

# Lemmatize, lowercase, and remove duplicates
words = sorted(set(lemmatizer.lemmatize(w.lower()) for w in words if w not in ['?', '!', '.', ',']))
classes = sorted(classes)

# Tokenize and lemmatize training sentences
training_sentences = [
    [lemmatizer.lemmatize(word.lower()) for word in re.findall(r'\b\w+\b', sentence)]
    for sentence in training_sentences
]

# Create bag-of-words representation
training_sentences_bow = []
for sentence in training_sentences:
    bow = [1 if word in sentence else 0 for word in words]
    training_sentences_bow.append(bow)

# Encode labels
label_encoder = LabelEncoder()
training_labels = label_encoder.fit_transform(training_labels)

# Convert to numpy arrays
training_labels = np.array(training_labels)
training_sentences_bow = np.array(training_sentences_bow)

# Pad sequences to ensure uniform length
max_length = 86  # Use fixed length of 86 based on your JavaScript code
training_sentences_bow = pad_sequences(training_sentences_bow, maxlen=max_length, padding='post')

# Reshape data for LSTM input (LSTM expects 3D input: [samples, time steps, features])
training_sentences_bow = np.expand_dims(training_sentences_bow, axis=-1)  # Add an extra dimension for features

# Split dataset into training and validation
X_train, X_val, y_train, y_val = train_test_split(training_sentences_bow, training_labels, test_size=0.2, random_state=42)

# Define the model with LSTM layer
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(max_length, 1)),  # LSTM layer with input shape
    Dropout(0.2),
    BatchNormalization(),
    LSTM(64, return_sequences=False),  # Second LSTM layer
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(classes), activation='softmax')  # Number of output classes (tags)
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
model.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])

# Save the model
model.save('chatbot_model.h5')

# Save supporting data for inference
pickle.dump(words, open('words.pkl', 'wb'))
pickle.dump(classes, open('classes.pkl', 'wb'))
pickle.dump(label_encoder, open('label_encoder.pkl', 'wb'))

print("Model and data saved successfully.")

#convertion to js 
!tensorflowjs_converter --input_format=keras /content/chatbot_model.h5 /content/model